<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <title>TP5 – Exploration, réglage des hyperparamètres et convergence</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      margin: 20px;
      line-height: 1.6;
    }
    h1, h2, h3 {
      color: #1a4e8a;
    }
    code {
      font-family: "Fira Code", "Consolas", monospace;
      background-color: #f5f5f5;
      padding: 2px 4px;
      border-radius: 4px;
    }
    pre {
      background-color: #f5f5f5;
      padding: 10px;
      border-radius: 6px;
      overflow-x: auto;
    }
    .box {
      border-left: 4px solid #1a4e8a;
      padding-left: 10px;
      margin: 15px 0;
      background: #f8fbff;
    }
    ul, ol {
      margin-left: 22px;
    }
    table {
      border-collapse: collapse;
      margin: 10px 0;
    }
    th, td {
      padding: 6px 10px;
    }
  </style>
</head>
<body>

<h1>TP5 – Exploration, réglage des hyperparamètres et convergence</h1>

<div class="box">
  <h2>Objectif pédagogique</h2>
  <p>
    Dans ce TP, vous allez analyser comment les <strong>hyperparamètres</strong> du Q-Learning
    influencent la qualité et la vitesse de l'apprentissage de l'agent.
  </p>
  <p>
    Vous utiliserez l'agent Q-Learning du TP4 et ferez varier :
    <strong>α</strong> (taux d'apprentissage), <strong>γ</strong> (discount factor) et
    <strong>ε</strong> (exploration).
  </p>
</div>

<h2>1. Rôle des hyperparamètres</h2>

<ul>
  <li><strong>α (alpha)</strong> : contrôle la vitesse d'apprentissage. Valeur élevée = l'agent réagit fortement aux nouvelles expériences.</li>
  <li><strong>γ (gamma)</strong> : pondère l'importance du futur. Proche de 1 = l'agent prend fortement en compte les récompenses futures.</li>
  <li><strong>ε (epsilon)</strong> : probabilité d'explorer au lieu d'exploiter. Au début, on souhaite souvent beaucoup d'exploration, puis réduire progressivement.</li>
</ul>

<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Paramètre</th>
    <th>Trop bas</th>
    <th>Trop haut</th>
  </tr>
  <tr>
    <td>α</td>
    <td>Apprentissage très lent</td>
    <td>Apprentissage instable</td>
  </tr>
  <tr>
    <td>γ</td>
    <td>Néglige le futur</td>
    <td>Survalorise le futur (peut rendre l'apprentissage instable)</td>
  </tr>
  <tr>
    <td>ε</td>
    <td>Trop peu d'exploration (risque de rester bloqué)</td>
    <td>Trop aléatoire, l'agent n'exploite pas ce qu'il a appris</td>
  </tr>
</table>

<h2>2. Expérience à réaliser</h2>

<p>
Vous allez réaliser plusieurs entraînements d'un agent sur le labyrinthe avec des configurations différentes
de <code>α</code>, <code>γ</code> et de plan d'exploration <code>ε</code>, puis comparer les courbes de
récompenses cumulées.
</p>

<h3>2.1. Structure générale du code</h3>

<p>Vous pouvez réutiliser la boucle d'entraînement du TP4, en y ajoutant une décroissance de ε :</p>

<pre><code>for episode in range(n_episodes):
    state = env.reset()
    total_reward = 0
    done = False

    # Exemple de décroissance de epsilon (exploration)
    agent.epsilon = max(0.1, agent.epsilon * 0.99)

    for step in range(max_steps):
        action = agent.choose_action(state)
        next_state, reward, done, _ = env.step(action)
        agent.update(state, action, reward, next_state)
        state = next_state
        total_reward += reward
        if done:
            break

    rewards_per_episode.append(total_reward)</code></pre>

<div class="box">
  <h3>Explication</h3>
  <ul>
    <li>À chaque épisode, on réduit légèrement <code>epsilon</code>.</li>
    <li>Au début, l'agent explore beaucoup (ε proche de 1).</li>
    <li>Au fil du temps, ε se rapproche de 0.1, ce qui favorise l'exploitation des meilleures actions apprises.</li>
  </ul>
</div>

<h3>2.2. Scénarios de test</h3>

<p>
Testez au moins les combinaisons suivantes :
</p>
<ul>
  <li>α ∈ (0.1, 0.5, 0.9)</li>
  <li>γ ∈ (0.5, 0.9, 0.99)</li>
</ul>

<p>
Pour chaque combinaison (<code>alpha, gamma</code>), entraînez un agent sur un nombre suffisant d'épisodes (par exemple 300 à 500)
et tracez la courbe des récompenses cumulées.
</p>

<h2>3. Visualisation des résultats</h2>

<p>Pour tracer une courbe de récompenses cumulées, vous pouvez utiliser :</p>

<pre><code>import matplotlib.pyplot as plt

plt.plot(rewards_per_episode)
plt.xlabel(&quot;Épisode&quot;)
plt.ylabel(&quot;Récompense totale&quot;)
plt.title(f&quot;α={alpha}, γ={gamma}, ε initial={epsilon_init}&quot;)
plt.grid(True)
plt.show()</code></pre>

<p>
Vous pouvez soit afficher une fenêtre graphique par configuration, soit enregistrer les courbes
dans des fichiers images pour les coller dans votre compte-rendu.
</p>

<h2>4. Expérimentations</h2>

<ol>
  <li>Choisir au moins 3 valeurs différentes de <code>alpha</code>, 2 valeurs de <code>gamma</code>.</li>
  <li>Mettre en place une décroissance de <code>epsilon</code> (par exemple : de 1.0 vers 0.1).</li>
  <li>Pour chaque configuration, lancer un entraînement complet et tracer la courbe.</li>
</ol>

<h2>5. Questions </h2>

<ul>

  <li>Quel est l'effet d'un <code>gamma</code> proche de 0.5 vs 0.99 sur le comportement de l'agent&nbsp;?</li>
  <li>Comment la décroissance de <code>epsilon</code> influence-t-elle la phase d'exploration puis d'exploitation&nbsp;?</li>
</ul>

<p>
Ce TP a pour but de vous faire comprendre qu'<strong>il n'existe pas un seul réglage magique</strong> des hyperparamètres,
et que l'ingénierie en apprentissage par renforcement consiste souvent à <strong>expérimenter, observer et ajuster</strong>.
</p>

</body>
</html>
