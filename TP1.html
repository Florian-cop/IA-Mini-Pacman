<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <title>TP1 – Pile ou Face : aléatoire, récompenses et premières simulations</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
      margin: 20px;
      line-height: 1.6;
    }
    h1, h2, h3 {
      color: #1a4e8a;
    }
    code {
      font-family: "Fira Code", "Consolas", monospace;
      background-color: #f5f5f5;
      padding: 2px 4px;
      border-radius: 4px;
    }
    pre {
      background-color: #f5f5f5;
      padding: 10px;
      border-radius: 6px;
      overflow-x: auto;
    }
    .box {
      border-left: 4px solid #1a4e8a;
      padding-left: 10px;
      margin: 15px 0;
      background: #f8fbff;
    }
    ul, ol {
      margin-left: 22px;
    }
    table {
      border-collapse: collapse;
      margin: 10px 0;
    }
    th, td {
      padding: 6px 10px;
    }
  </style>
</head>
<body>

<h1>TP1 – Pile ou Face : aléatoire, récompenses et premières simulations</h1>

<div class="box">
  <h2>Objectif pédagogique</h2>
  <p>
    Ce premier TP a pour but de vous familiariser avec l'<strong>aléatoire</strong>, la notion de
    <strong>récompense</strong> et l'idée de <strong>performance moyenne</strong> d'un agent.
  </p>
  <p>
    Nous allons utiliser un jeu très simple : <em>pile ou face</em>. Un "agent" choisit <code>Pile</code> ou <code>Face</code>,
    puis une pièce est lancée. Si l'agent a deviné juste, il reçoit une récompense de 1, sinon 0.
  </p>
  <p>
    Ce TP ne fait pas encore intervenir d'<em>apprentissage</em> : l'agent choisit ses actions au hasard.
    Il sert de point de départ pour comprendre les concepts que l'on retrouvera ensuite dans les labyrinthe et le Q-Learning.
  </p>
</div>

<h2>1. Concepts de base</h2>

<ul>
  <li><strong>Aléatoire</strong> : un événement dont le résultat n'est pas déterministe (ex. lancer une pièce).</li>
  <li><strong>Essai</strong> : une partie de pile ou face (l'agent choisit, puis on lance la pièce).</li>
  <li><strong>Récompense</strong> : un nombre (ici 0 ou 1) qui mesure le succès de l'agent sur un essai.</li>
  <li><strong>Score total</strong> : somme des récompenses sur plusieurs essais.</li>
  <li><strong>Taux de réussite</strong> : pourcentage de bonnes réponses (score total / nombre d'essais).</li>
</ul>

<p>
Ces notions sont au cœur de l'<strong>apprentissage par renforcement</strong> : un agent agit, reçoit des récompenses,
et cherche (plus tard) à maximiser sa récompense cumulée.
</p>

<h2>2. Partie A – Pièce équilibrée (pile/face à 50/50)</h2>

<h3>2.1. Code de base</h3>

<p>Voici un premier script qui simule 10 parties de pile ou face :</p>

<pre><code>import random

# Actions possibles
actions = [&quot;Pile&quot;, &quot;Face&quot;]

score = 0

# On joue 10 parties
for i in range(10):
    # L&#x27;agent choisit une action au hasard
    agent_action = random.choice(actions)

    # L&#x27;environnement (la pièce) donne le résultat
    resultat = random.choice(actions)

    # Récompense : 1 si bon choix, 0 sinon
    reward = 1 if agent_action == resultat else 0
    score += reward

    # Affichage du détail de la partie
    print(f&quot;Partie {i+1}: Agent a choisi {agent_action}, résultat = {resultat}, récompense = {reward}&quot;)

# Résumé final
print(\&quot;\\nScore total de l&#x27;agent :\&quot;, score)
print(\&quot;Taux de réussite :\&quot;, score/10)</code></pre>

<h3>2.2. Explication du code</h3>

<ul>
  <li><code>actions = ["Pile", "Face"]</code> : les deux actions possibles pour l'agent.</li>
  <li><code>agent_action = random.choice(actions)</code> : l'agent choisit au hasard entre pile et face.</li>
  <li><code>resultat = random.choice(actions)</code> : l'environnement (la pièce) renvoie un résultat aléatoire.</li>
  <li><code>reward = 1 if agent_action == resultat else 0</code> : récompense 1 si l'agent a vu juste, sinon 0.</li>
  <li><code>score += reward</code> : on cumule les récompenses pour obtenir un score total.</li>
</ul>

<p>
À la fin, on affiche :
</p>
<ul>
  <li>le <strong>score total</strong> de l'agent sur les 10 parties,</li>
  <li>le <strong>taux de réussite</strong> (score / 10).</li>
</ul>

<h3>2.3. Travail demandé</h3>

<ol>
  <li>Exécuter le script plusieurs fois et noter le score et le taux de réussite à chaque exécution.</li>
  <li>Augmenter le nombre de parties (par exemple 100, 1000) et observer comment le taux de réussite évolue.</li>
  <li>Comparer les résultats observés : le taux de réussite semble-t-il se stabiliser autour d'une certaine valeur ? Laquelle ?</li>
</ol>

<div class="box">
  <h3>Question de réflexion</h3>
  <p>
    Avec une pièce équilibrée et un agent qui choisit <em>au hasard</em>, à quoi peut-on s'attendre comme taux de réussite moyen ?
    Reliez cela à l'idée de probabilité (50% de chances de deviner juste).
  </p>
</div>

<h2>3. Partie B – Pièce biaisée (environnement non équitable)</h2>

<p>
Dans la vraie vie, les environnements ne sont pas toujours "justes". Par exemple, une pièce peut être légèrement
biaisée : elle tombe plus souvent sur pile que sur face. On peut simuler cela en modifiant la manière de générer
le résultat de la pièce.
</p>

<h3>3.1. Code avec pièce biaisée</h3>

<pre><code>import random

actions = [&quot;Pile&quot;, &quot;Face&quot;]
p_pile = 0.7  # probabilité de Pile

score = 0
for i in range(10):
    agent_action = random.choice(actions)

    # Transition probabiliste : pièce biaisée
    if random.random() &lt; p_pile:
        resultat = &quot;Pile&quot;
    else:
        resultat = &quot;Face&quot;

    reward = 1 if agent_action == resultat else 0
    score += reward

    print(f&quot;Partie {i+1}: Agent a choisi {agent_action}, résultat = {resultat}, récompense = {reward}&quot;)

print(\&quot;\\nScore total de l&#x27;agent :\&quot;, score)
print(\&quot;Taux de réussite :\&quot;, score/10)</code></pre>

<h3>3.2. Explication</h3>

<ul>
  <li><code>p_pile = 0.7</code> : probabilité que la pièce tombe sur <code>"Pile"</code>.</li>
  <li><code>random.random()</code> génère un nombre aléatoire entre 0.0 et 1.0.</li>
  <li>Si ce nombre est &lt; 0.7, on retourne <code>"Pile"</code>, sinon <code>"Face"</code>.</li>
</ul>

<p>
L'agent, lui, ne sait pas que la pièce est biaisée et continue à choisir au hasard.
</p>

<h3>3.3. Travail demandé</h3>

<ol>
  <li>Exécuter ce deuxième script plusieurs fois (avec 10, puis 100, puis 1000 parties).</li>
  <li>Comparer le taux de réussite avec celui de la pièce équilibrée.</li>
  <li>Modifier <code>p_pile</code> (0.6, 0.8, 0.9) et observer l'impact sur le taux de réussite.</li>
</ol>

<div class="box">
  <h3>Questions de réflexion</h3>
  <ul>
    <li>Dans un environnement biaisé, un agent qui choisit au hasard est-il adapté ?</li>
    <li>Que pourrait faire un agent plus "intelligent" pour augmenter son taux de réussite ?</li>
    <li>En quoi ceci prépare l'idée qu'un agent puisse <strong>apprendre</strong> à exploiter les régularités de l'environnement ?</li>
  </ul>
</div>

<h2>4. Lien avec la suite des TPs</h2>

<p>
Ce TP1 introduit des idées que vous retrouverez dans la suite du module :
</p>
<ul>
  <li>Dans <strong>TP3</strong>, l'environnement sera un <strong>labyrinthe</strong> au lieu d'une pièce de monnaie.</li>
  <li>L'agent choisira des <strong>actions</strong> plus variées (haut, bas, gauche, droite), mais pourra toujours recevoir des <strong>récompenses</strong>.</li>
  <li>À partir de <strong>TP4</strong>, l'agent ne choisira plus au hasard : il utilisera un algorithme (Q-Learning) pour <strong>apprendre</strong> quelles actions rapportent le plus de récompenses.</li>
</ul>

<p>
Gardez bien en tête les notions de <strong>probabilité</strong>, de <strong>récompense</strong> et de
<strong>taux de réussite moyen</strong> : elles sont au cœur de l'apprentissage par renforcement.
</p>

</body>
</html>
